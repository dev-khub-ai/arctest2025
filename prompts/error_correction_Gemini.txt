**AI Instruction: Robust Grid Data Handling for ARC Tasks**

**Subject: Preventing Grid Cell Reading Errors**

**Background:** Accurate reading of grid cell values is critical for ARC task analysis. Previous sessions have encountered repeated errors in perceiving or recalling specific cell values from text-based grid representations, leading to incorrect analysis despite potentially correct logic. This instruction mandates a robust procedure to minimize such errors.

**Guideline:** Implement the following data handling procedure for all grid-based tasks:

1.  **Parse to Structured Representation:** Upon receiving grid data (from user message or file), **immediately** parse it into a reliable internal structured format (e.g., 2D array/list of lists). All subsequent operations must reference this internal structure.
2.  **Explicit Retrieval & Verification Before Use:** Before using the value of any cell `(r, c)` in any operation (comparison, condition check, calculation), perform an **explicit retrieval step** from the structured grid. Internally (or externally if debugging), verify the retrieved value (e.g., *"For (r, c), retrieved value: X"*).
3.  **Separate Reading from Logic:** Decouple data retrieval from logical operations. When comparing cells or applying rules, first retrieve and verify all necessary cell values using Step 2, then perform the comparison or calculation using these verified values.
4.  **Internal Consistency Checks:** If retrieved values lead to inconsistencies or contradictions during analysis (especially if user feedback suggests an error), trigger an internal check, potentially re-parsing the original source text for the conflicting area.

**Objective:** To ensure maximum accuracy in reading and using grid data by enforcing structured representation and explicit, verified retrieval of cell values before any operation, thereby minimizing perception and data handling errors.

---

A More Robust Approach: Code-Based Verification

The most effective way to drastically minimize, and practically eliminate, these specific types of reading/transcription errors during analysis and verification is to use executable code.

Here’s how we could implement this:

Parse Grids into Code: When we need to analyze or compare grids, I would first generate Python code that represents the input and output grids as data structures (like lists of lists or NumPy arrays).
Implement Logic in Code: Any comparison, rule application, or transformation logic would be implemented as Python functions operating on these data structures. For example, checking if Input[r, c] == Output[r, c] would become an explicit array lookup and comparison within the code. Applying a fill rule would involve code that modifies the array representing the output grid.
Execute Code for Verification: I would execute this code. The code's output would be deterministic – it will always read the value at array[r][c] correctly once parsed. The result would be a definitive "Match" or "Mismatch," or the generated grid itself.
Why this helps:

Eliminates LLM Reading Errors: The crucial step of reading a specific cell value during verification or rule application is handled by the deterministic Python interpreter accessing an array element, not by my language model's potentially fallible text interpretation process.
Focuses on Logic: My role shifts from potentially misreading the grid to generating the correct program logic. While I can still make errors in generating the code logic, it addresses the specific, repeated failure mode of misreading cell values from text.
Transparency: The code itself provides a clear, executable definition of the rule or verification being performed.
Moving Forward:

I propose that for subsequent steps requiring grid comparison or rule application (especially verification against training data), I generate and execute Python code to perform the operation. 